{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T01:00:06.637128Z",
     "start_time": "2021-04-08T01:00:06.623132Z"
    }
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery, storage\n",
    "from datetime import date, datetime, timedelta\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "import fileinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:47:18.307917Z",
     "start_time": "2021-04-07T21:47:18.295884Z"
    }
   },
   "outputs": [],
   "source": [
    "GCP_CREDENTIALS = os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]\n",
    "GCP_CREDENTIALS\n",
    "BQ_CLIENT = bigquery.Client.from_service_account_json(GCP_CREDENTIALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:47:19.296640Z",
     "start_time": "2021-04-07T21:47:19.292643Z"
    }
   },
   "outputs": [],
   "source": [
    "entities_bucket = 'peya-great-expectations-project-pro'\n",
    "entities_prefix = 'entities/logistics/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T01:47:41.005898Z",
     "start_time": "2021-04-08T01:47:40.998896Z"
    }
   },
   "outputs": [],
   "source": [
    "def upload_yml_file_from_template(bucket, file_prefix, template, variable, item):\n",
    "    \n",
    "    new_file_prefix = r'{prefix}_{var}_{item}.yaml'.format(prefix=file_prefix, var=variable, item=item)\n",
    "    filename = new_file_prefix.split('/')[-1]\n",
    "    \n",
    "    #testing\n",
    "    filename = r'C:\\Users\\carlos.jaime\\Repositories\\data-dataquality-dags\\airflow\\dags\\DATA_QUALITY_Framework\\entities\\Logistics\\{}'.format(filename) \n",
    "    \n",
    "    try:\n",
    "        os.remove(filename)\n",
    "        print('file removed')\n",
    "    except:\n",
    "        print('file not exists')\n",
    "    \n",
    "    with open(filename, 'w') as file:\n",
    "        documents = yaml.dump(template, file)\n",
    "        \n",
    "    with fileinput.FileInput(filename, inplace=True) as file:\n",
    "        for line in file:\n",
    "            print(line.replace('_{}_'.format(variable),item), end='')\n",
    "    \n",
    "    # Upload generated files\n",
    "    try: \n",
    "        # Upload new file\n",
    "        storage_client = storage.Client.from_service_account_json(GCP_CREDENTIALS)\n",
    "\n",
    "        # Entities bucket\n",
    "        client_bucket = storage_client.get_bucket(bucket)\n",
    "    \n",
    "        blob = client_bucket.blob(new_file_prefix)\n",
    "        blob.upload_from_filename(filename)\n",
    "     \n",
    "        print('file uploaded')\n",
    "    except:\n",
    "        print('error')\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T12:23:14.297812Z",
     "start_time": "2021-04-08T12:23:14.288816Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_template_files(entities_bucket, entities_prefix):\n",
    "    \n",
    "    # Required Google Storage Credentials\n",
    "    storage_client = storage.Client.from_service_account_json(GCP_CREDENTIALS)\n",
    "\n",
    "    # Entities bucket\n",
    "    client_bucket = storage_client.get_bucket(entities_bucket)\n",
    "\n",
    "    # # Iterate over bucket and prefix to locate the entity's .yaml files\n",
    "    for blob in storage_client.list_blobs(client_bucket, prefix=entities_prefix):\n",
    "        if str(blob.name).endswith('.yaml'):\n",
    "\n",
    "            blob = client_bucket.get_blob(blob.name)\n",
    "\n",
    "            if str(blob.name).endswith('TEMPLATE.yaml'):\n",
    "                \n",
    "                            \n",
    "                destination_uri = '{}/{}'.format(r'C:\\Users\\carlos.jaime\\Repositories\\data-dataquality-dags\\airflow\\dags\\DATA_QUALITY_Framework\\entities\\Logistics', blob.name)\n",
    "                blob.download_to_filename(destination_uri)\n",
    "                \"\"\"\n",
    "                now = datetime.now()\n",
    "            \n",
    "                duration = blob.updated.replace(tzinfo=None) - now        \n",
    "                duration_in_s = duration.total_seconds()\n",
    "                minutes = divmod(duration_in_s, 60)[0] \n",
    "                \n",
    "                print(minutes)\n",
    "                print(now)\n",
    "                print(blob.updated.replace(tzinfo=None))\n",
    "                \n",
    "                # Check if TEMPLATE file was changeat least 5\n",
    "                if minutes < 5:\n",
    "                    print('Updated yamls files...')\n",
    "\n",
    "                \n",
    "                    blob_file = yaml.load(blob.download_as_string(), Loader=yaml.FullLoader)\n",
    "\n",
    "                    file_prefix = blob.name.split('_TEMPLATE')[0]\n",
    "                    print(file_prefix)\n",
    "\n",
    "                    configuration = blob_file.get('configuration') # Definition of templates\n",
    "                    template = blob_file.get('template') # To generate Files\n",
    "\n",
    "                    if configuration:\n",
    "                        description = configuration.get('description')\n",
    "                        variables = configuration.get('variables',{})\n",
    "                        for var in variables:\n",
    "                            print(var)\n",
    "                            for key in var:\n",
    "                                variable = key\n",
    "                                values = var[key]\n",
    "\n",
    "                                if type(values) == str:\n",
    "                                    print('{} is a Str'.format(variable))\n",
    "                                    #ToDo logica de generar archivos\n",
    "                                elif type(values) == list:\n",
    "                                    print('{} is a list'.format(variable))\n",
    "                                    for item in values:\n",
    "                                        print(item)\n",
    "\n",
    "                                        upload_yml_file_from_template(bucket=entities_bucket,\n",
    "                                                                     file_prefix=file_prefix,\n",
    "                                                                     template=template,\n",
    "                                                                     variable=variable,\n",
    "                                                                     item=item)\n",
    "\n",
    "                                        #ToDo logica de generar archivos\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T12:23:15.604408Z",
     "start_time": "2021-04-08T12:23:14.712915Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_template_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-43e71771358d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerate_template_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities_bucket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentities_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'generate_template_files' is not defined"
     ]
    }
   ],
   "source": [
    "generate_template_files(entities_bucket, entities_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:47:55.359505Z",
     "start_time": "2021-04-07T21:47:55.349506Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'template' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-cd5315ab5bb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'template' is not defined"
     ]
    }
   ],
   "source": [
    "new_file = r'C:\\Users\\carlos.jaime\\Repositories\\data-dataquality-dags\\airflow\\dags\\DATA_QUALITY_Framework\\entities\\Logistics\\store_files.yaml'\n",
    "\n",
    "\n",
    "\n",
    "with open(new_file, 'w') as file:\n",
    "    documents = yaml.dump(template, file, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T18:53:01.413754Z",
     "start_time": "2021-04-08T18:53:01.344894Z"
    }
   },
   "outputs": [],
   "source": [
    "datetime.strftime(datetime.now() - timedelta(1), '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #print(file)\n",
    "    file = file.split('/')[0]\n",
    "    \n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tasks_files_groups_dict(file, prefix='/', task_files={}):\n",
    "    # Itera sobre el blob.name y define arbol de ejecucipon de los jobs\n",
    "    # de acuerdo al prefix de cada archivo .yaml\n",
    "    file_split = file.split('/') # list folders and file\n",
    "    \n",
    "    # Es un archivo\n",
    "    if len(file_split)==1:\n",
    "        # genera job\n",
    "        file_to_job = ('{}/{}'.format(prefix, file_split[-1])).replace('//','/')\n",
    "        \n",
    "        dummy_task = prefix.replace('/','_')\n",
    "        \n",
    "        # Genera/actualiza diccionario con job\n",
    "        if task_files.get(dummy_task):\n",
    "            task_files[dummy_task].append(file_to_job)\n",
    "        else:\n",
    "            task_files[dummy_task] = [file_to_job]\n",
    "    \n",
    "    # es un prefix de archivo\n",
    "    else:    \n",
    "        sub_file = '/'.join(file_split[1:])\n",
    "        sub_prefix = '{prefix}/{sub_prefix}'.format(prefix=prefix,sub_prefix=file_split[0]).replace(' ','')\n",
    "        # Itera nuevamente\n",
    "        generate_tasks_files_groups_dict(file=sub_file,prefix=sub_prefix, task_files=task_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities/logistics/cl_logistics_COUNTRY_CODE_AR.yaml\n",
      "entities/logistics/cl_logistics_COUNTRY_CODE_BO.yaml\n",
      "entities/logistics/cl_logistics_COUNTRY_CODE_CL.yaml\n",
      "entities/logistics/cl_logistics_COUNTRY_CODE_CO.yaml\n",
      "entities/logistics/cl_logistics_COUNTRY_CODE_CR.yaml\n",
      "entities/logistics/cl_logistics_COUNTRY_CODE_DO.yaml\n",
      "entities/logistics/cl_logistics_COUNTRY_CODE_EC.yaml\n",
      "entities/logistics/cl_logistics_COUNTRY_CODE_GT.yaml\n",
      "entities/logistics/cl_logistics_COUNTRY_CODE_HN.yaml\n",
      "entities/logistics/cl_logistics_COUNTRY_CODE_PA.yaml\n",
      "entities/logistics/cl_logistics_COUNTRY_CODE_PE.yaml\n",
      "entities/logistics/cl_logistics_COUNTRY_CODE_PY.yaml\n",
      "entities/logistics/cl_logistics_COUNTRY_CODE_SV.yaml\n",
      "entities/logistics/cl_logistics_COUNTRY_CODE_UY.yaml\n",
      "entities/logistics/cl_logistics_COUNTRY_CODE_VE.yaml\n",
      "entities/logistics/intelligence_layer/il_logistics.yaml\n",
      "entities/logistics/intelligence_layer/test_folder/store_files.yaml\n",
      "{'entities_logistics_': ['entities/logistics/cl_logistics_COUNTRY_CODE_AR.yaml', 'entities/logistics/cl_logistics_COUNTRY_CODE_BO.yaml', 'entities/logistics/cl_logistics_COUNTRY_CODE_CL.yaml', 'entities/logistics/cl_logistics_COUNTRY_CODE_CO.yaml', 'entities/logistics/cl_logistics_COUNTRY_CODE_CR.yaml', 'entities/logistics/cl_logistics_COUNTRY_CODE_DO.yaml', 'entities/logistics/cl_logistics_COUNTRY_CODE_EC.yaml', 'entities/logistics/cl_logistics_COUNTRY_CODE_GT.yaml', 'entities/logistics/cl_logistics_COUNTRY_CODE_HN.yaml', 'entities/logistics/cl_logistics_COUNTRY_CODE_PA.yaml', 'entities/logistics/cl_logistics_COUNTRY_CODE_PE.yaml', 'entities/logistics/cl_logistics_COUNTRY_CODE_PY.yaml', 'entities/logistics/cl_logistics_COUNTRY_CODE_SV.yaml', 'entities/logistics/cl_logistics_COUNTRY_CODE_UY.yaml', 'entities/logistics/cl_logistics_COUNTRY_CODE_VE.yaml'], 'entities_logistics__intelligence_layer': ['entities/logistics/intelligence_layer/il_logistics.yaml'], 'entities_logistics__intelligence_layer_test_folder': ['entities/logistics/intelligence_layer/test_folder/store_files.yaml']}\n"
     ]
    }
   ],
   "source": [
    "# Required Google Storage Credentials\n",
    "storage_client = storage.Client.from_service_account_json(GCP_CREDENTIALS)\n",
    "\n",
    "# Entities bucket\n",
    "client_bucket = storage_client.get_bucket(entities_bucket)\n",
    "\n",
    "task_files = {}\n",
    "\n",
    "# # Iterate over bucket and prefix to locate the entity's .yaml files\n",
    "for blob in storage_client.list_blobs(client_bucket, prefix=entities_prefix):\n",
    "    if str(blob.name).endswith('.yaml') and not (str(blob.name).endswith('TEMPLATE.yaml')): \n",
    "        \n",
    "        print(blob.name)\n",
    "        \n",
    "        blob_file = blob.name.split(entities_prefix)[-1]\n",
    "        \n",
    "        generate_tasks_files_groups_dict(blob_file, prefix=entities_prefix, task_files=task_files)\n",
    "\n",
    "print(task_files)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in task_files.keys():\n",
    "    for job in task_files[key]:\n",
    "        print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['root'].append('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if x.get('otro'):\n",
    "    print('existe')\n",
    "else:\n",
    "    x['otro'] = ['otro mas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = ['1','2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tasks = ['3','4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tasks_2 = ['5','6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.extend([sub_tasks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "config_path_filename = r'C:\\Users\\carlos.jaime\\Repositories\\data-dataquality-dags\\airflow\\dags\\DATA_QUALITY_Framework\\config\\logistics_checkpoint_config.yaml'\n",
    "\n",
    "GE_GCS_STORE_BACKEND = '{ \"default_bucket_name\": \"peya-great-expectations-project-pro\", \"default_project_name\": \"peya-data-qlty-pro\", \"validations_store_prefix\": \"great_expectations_context/validations\", \"expectations_store_prefix\": \"great_expectations_context/expectations\", \"data_docs_prefix\": \"great_expectations_context/data_docs\", \"checkpoints_store_prefix\": \"great_expectations_context/checkpoints\", \"parameters_store_prefix\": \"great_expectations_context/parameters\" }'\n",
    "\n",
    "GE_GCS_STORE_BACKEND_DICT = json.loads(GE_GCS_STORE_BACKEND)\n",
    "\n",
    "with open(config_path_filename) as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    CONFIG_YAML = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    validator_config = CONFIG_YAML.get('validator_config')\n",
    "    entities_repo = validator_config.get('entities_repository')\n",
    "\n",
    "    ENTITIES_BUCKET = entities_repo['bucket']\n",
    "    ENTITIES_PREFIX = entities_repo['prefix']\n",
    "\n",
    "    # check overwrite default store backend config\n",
    "    if validator_config.get('store_backend'):\n",
    "        store_backend = validator_config['store_backend']\n",
    "        # overwrite store backend for validations results\n",
    "        if store_backend.get('validations_store_prefix'):\n",
    "\n",
    "            default_validation = GE_GCS_STORE_BACKEND_DICT['validations_store_prefix']\n",
    "            new_validation = store_backend['validations_store_prefix']\n",
    "\n",
    "            GE_GCS_STORE_BACKEND_DICT['validations_store_prefix'] = '{default}/{new}'.format(default=default_validation,\n",
    "                                                                                        new=new_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "GE_GCS_STORE_BACKEND = '{}'.format(GE_GCS_STORE_BACKEND_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'default_bucket_name': 'peya-great-expectations-project-pro', 'default_project_name': 'peya-data-qlty-pro', 'validations_store_prefix': 'great_expectations_context/validations/logistics', 'expectations_store_prefix': 'great_expectations_context/expectations', 'data_docs_prefix': 'great_expectations_context/data_docs', 'checkpoints_store_prefix': 'great_expectations_context/checkpoints', 'parameters_store_prefix': 'great_expectations_context/parameters'}\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GE_GCS_STORE_BACKEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
