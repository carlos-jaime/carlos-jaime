{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install google-cloud-bigquery-datatransfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T17:43:26.240450Z",
     "start_time": "2021-05-31T17:43:25.253002Z"
    }
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery, storage\n",
    "import pydata_google_auth\n",
    "from google.api_core.exceptions import AlreadyExists, NotFound, BadRequest, Forbidden \n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T17:43:26.498250Z",
     "start_time": "2021-05-31T17:43:26.253096Z"
    }
   },
   "outputs": [],
   "source": [
    "SCOPES = [\n",
    "    'https://www.googleapis.com/auth/cloud-platform',\n",
    "    'https://www.googleapis.com/auth/bigquery',\n",
    "]\n",
    "\n",
    "user_credentials = pydata_google_auth.get_user_credentials(\n",
    "    SCOPES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T17:43:26.513082Z",
     "start_time": "2021-05-31T17:43:26.510080Z"
    }
   },
   "outputs": [],
   "source": [
    "# Projecto donde se va a ejecutar el job\n",
    "PROJECT_ID = 'peya-data-qlty-stg'\n",
    "\n",
    "TEMPORAL_PROJECT_ID = 'peya-data-qlty-stg'\n",
    "TEMPORAL_DATASET_ID = 'data_export_temporal'\n",
    "\n",
    "MAX_BYTES_BILLED = 5368709120 # 5GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-31T17:43:26.708735Z",
     "start_time": "2021-05-31T17:43:26.705734Z"
    }
   },
   "outputs": [],
   "source": [
    "BIGQUERY_CLIENT = bigquery.Client(credentials=user_credentials, project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIGQUERY_CLIENT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solicitud de exportación de data\n",
    "USER_EMAIL = 'carlos.jaime@pedidosya.com'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuente de los datos, tabla o query\n",
    "USER_QUERY = \"\"\"\n",
    "select order_id, o.business_type.business_type_id, o.business_type.business_type_name, o.amount_no_discount, o.commission, o.addressDescription\n",
    "from `peya-bi-tools-pro.il_core.fact_orders` o\n",
    "where registered_date >= '2020-01-01'\n",
    "limit 100000000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def humanbytes(B):\n",
    "   'Return the given bytes as a human friendly KB, MB, GB, or TB string'\n",
    "   B = float(B)\n",
    "   KB = float(1024)\n",
    "   MB = float(KB ** 2) # 1,048,576\n",
    "   GB = float(KB ** 3) # 1,073,741,824\n",
    "   TB = float(KB ** 4) # 1,099,511,627,776\n",
    "\n",
    "   if B < KB:\n",
    "      return '{0} {1}'.format(B,'Bytes' if 0 == B > 1 else 'Byte')\n",
    "   elif KB <= B < MB:\n",
    "      return '{0:.2f} KB'.format(B/KB), float(B/KB)\n",
    "   elif MB <= B < GB:\n",
    "      return '{0:.2f} MB'.format(B/MB), float(B/MB)\n",
    "   elif GB <= B < TB:\n",
    "      return '{0:.2f} GB'.format(B/GB), float(B/GB)\n",
    "   elif TB <= B:\n",
    "      return '{0:.2f} TB'.format(B/TB), float(B/TB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom methods \n",
    "class MaximumBytesExceeded(Exception):\n",
    "    pass\n",
    "\n",
    "class QueryStatementNotAllowed(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuente de los datos, tabla o query\n",
    "USER_QUERY_TEST = \"\"\"\n",
    "SELECT * FROM `peya-uruguay.user_maria_elola.Reporte_Febrero_2021`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_dry_run(**kwargs):\n",
    "    \"\"\"\n",
    "    Check different parameters to return True to continue execution\n",
    "    \"\"\"\n",
    "    # custom result dict\n",
    "    query_dry_run_results = {'success':False,\n",
    "                            'message':None}\n",
    "    \n",
    "    MAX_BYTES_PROCESSED = kwargs.get('max_bytes_billed',26843545600) # Default 25GB\n",
    "    QUERY = kwargs.get('query')\n",
    "    BQ_CLIENT = kwargs.get('client')\n",
    "    \n",
    "    assert (QUERY), 'Query parameter cannot be null'\n",
    "    \n",
    "    # Job config to validate query billing, rows, slots\n",
    "    job_test_config = bigquery.QueryJobConfig(dry_run=True, \n",
    "                                              use_query_cache=False)\n",
    "    \n",
    "    try:\n",
    "        # Check \n",
    "        job_query_test = BQ_CLIENT.query(query=QUERY,\n",
    "                                           job_id_prefix='DataTech_ExportDataService_DryRun_Query',\n",
    "                                           job_config=job_test_config)\n",
    "        \n",
    "        print(job_query_test.allow_large_results)\n",
    "        \n",
    "        # check only \"SELECT\" statements can be executed\n",
    "        if job_query_test.statement_type != 'SELECT':\n",
    "            raise QueryStatementNotAllowed\n",
    "        \n",
    "        # Check billed limit\n",
    "        if job_query_test.total_bytes_processed >= MAX_BYTES_PROCESSED: \n",
    "            raise MaximumBytesExceeded\n",
    "            \n",
    "        # if all validations was ok\n",
    "        query_dry_run_results['message'] = \"This query will process {} \".format(humanbytes(job_query_test.total_bytes_processed))\n",
    "        query_dry_run_results['success'] = True\n",
    "\n",
    "    except MaximumBytesExceeded:\n",
    "        query_dry_run_results['message'] = 'ERROR: Query Exceeded the maximum number of bytes allowed (It will be processed: {} | Max. allowed: {})'.format(humanbytes(job_query_test.total_bytes_processed),\n",
    "                                                                                                                                                    humanbytes(MAX_BYTES_PROCESSED))\n",
    "    except QueryStatementNotAllowed: \n",
    "        query_dry_run_results['message'] = 'ERROR: Query Statement {} NOT allowed, only SELECT statements are allowed.'.format(job_query_test.statement_type)\n",
    "        \n",
    "    except Forbidden as f:\n",
    "        # Error trying to access to columns with policy tags\n",
    "        query_dry_run_results['message'] = 'ERROR: BigQuery Client Forbidden: {}'.format(f.message)\n",
    "    \n",
    "    except BadRequest as e:\n",
    "        # Error when the query has syntax problems\n",
    "        query_dry_run_results['message'] = 'ERROR: BigQuery Client BadRequest: {}'.format(e.message)\n",
    "        \n",
    "    except:\n",
    "        query_dry_run_results['message'] = 'ERROR: {}'.format(sys.exc_info()[0])\n",
    "        \n",
    "    return query_dry_run_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dry_run(query=USER_QUERY, client=BIGQUERY_CLIENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de tabla temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_table(**kwargs):\n",
    "    \"\"\"\n",
    "    Generate and export data from a table in google bigquery to a bucket and prefix in google storage.\n",
    "    :param\n",
    "        bigquery_client (bigquery.client): \n",
    "        user_email (str):\n",
    "        user_query (str): \n",
    "    \"\"\"\n",
    "    \n",
    "    # custom result dict\n",
    "    create_temporal_table_result = {'success':False,\n",
    "                                    'message':None,\n",
    "                                    'temporal_table_id':None}\n",
    "    \n",
    "    # method arguments\n",
    "    BQ_CLIENT = kwargs.get('bigquery_client')\n",
    "    USER_EMAIL = kwargs.get('user_email','DefaultUser')\n",
    "    USER_QUERY = kwargs.get('user_query')\n",
    "    TEMPORAL_PROJECT_ID = kwargs.get('temporal_project_id')\n",
    "    TEMPORAL_DATASET_ID = kwargs.get('temporal_dataset_id')\n",
    "    TEMPORAL_EXPIRATION_TABLE = kwargs.get('expiration', 120)\n",
    "    \n",
    "    MAX_BYTES_BILLED_ALLOWED = kwargs.get('max_bytes_billed_allowed',26843545600) # Default 25GB\n",
    "    \n",
    "    assert (USER_EMAIL and USER_QUERY and TEMPORAL_PROJECT_ID and TEMPORAL_DATASET_ID and BQ_CLIENT), 'Arguments cannot be null'\n",
    "    \n",
    "    # Format user name\n",
    "    names = USER_EMAIL.split('@')[0].split('.')\n",
    "    user_name = ''.join([name.capitalize() for name in names])\n",
    "    \n",
    "    # First, run a test to measure the number of bytes to process and if the sql script is ok\n",
    "    # Return a dict with results and messages\n",
    "    dry_run_result = query_dry_run(query=USER_QUERY, \n",
    "                                   client= BQ_CLIENT, \n",
    "                                   max_bytes_billed=MAX_BYTES_BILLED_ALLOWED)\n",
    "  \n",
    "    # The query is fine, proceed to create a temporary table\n",
    "    if dry_run_result.get('success',False):\n",
    "        \n",
    "        # Temporal Table Parameters\n",
    "        # Timestamp to temporal table name\n",
    "        current_timestamp = datetime.strftime(datetime.now(), \"%Y%m%d_%H%M00\")\n",
    "\n",
    "        # temporal table id\n",
    "        temporal_table_name = 'data_export_{user}_{timestamp}'.format(user=user_name, \n",
    "                                                                      timestamp=current_timestamp)\n",
    "        # full temporal table id\n",
    "        temporal_table_id = '{project}.{dataset}.{table_name}'.format(project=TEMPORAL_PROJECT_ID,\n",
    "                                                                             dataset=TEMPORAL_DATASET_ID,\n",
    "                                                                             table_name=temporal_table_name)\n",
    "\n",
    "\n",
    "        # Then, generate a new table, if exists drop it and create a new one\n",
    "        try:\n",
    "            table = BQ_CLIENT.get_table(temporal_table_id)\n",
    "            #Set new query to view\n",
    "            BQ_CLIENT.delete_table(table=temporal_table_id, not_found_ok=True)\n",
    "\n",
    "        except:\n",
    "\n",
    "            print('Creating temporal table {}'.format(temporal_table_name))\n",
    "\n",
    "        # Set configuration.query.destinationTable\n",
    "        destination_dataset = BQ_CLIENT.dataset(TEMPORAL_DATASET_ID)\n",
    "        destination_table = destination_dataset.table(temporal_table_name)\n",
    "\n",
    "        # https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.QueryJobConfig.html#google.cloud.bigquery.job.QueryJobConfig\n",
    "        job_config = bigquery.QueryJobConfig()\n",
    "        \n",
    "        # Destination table reference\n",
    "        job_config.destination = destination_table\n",
    "        # Set configuration.query.createDisposition\n",
    "        job_config.create_disposition = 'CREATE_IF_NEEDED'\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Start the query, it will create a temporal table\n",
    "            job = BQ_CLIENT.query(query=USER_QUERY,\n",
    "                                        job_id_prefix='DataTech_ExportDataService_Table', \n",
    "                                        job_config=job_config)\n",
    "\n",
    "            # Wait for the query to finish\n",
    "            job.result()\n",
    "            \n",
    "            assert job.state == 'DONE', 'Error when try to create the temporal table. job status result: {}'.format(job.state)\n",
    "            \n",
    "            # Update table metadata\n",
    "            dataset_ref = bigquery.DatasetReference(TEMPORAL_PROJECT_ID, TEMPORAL_DATASET_ID)\n",
    "            table_ref = dataset_ref.table(temporal_table_name)\n",
    "            table = bigquery.Table(table_ref)\n",
    "            \n",
    "                    \n",
    "            # Expiración de la tabla temporal\n",
    "            expiration = datetime.now(pytz.utc) + timedelta(minutes=TEMPORAL_EXPIRATION_TABLE)\n",
    "            # Descripción\n",
    "            description = 'Temporal Table generate to export data into GCS.\\nUser: {u}\\nData Tech | Export Data Service'.format(u=USER_EMAIL)\n",
    "            # Labels a nivel de la tabla\n",
    "            labels = {\"user\":user_name.lower(),\n",
    "                     \"service\":\"export_data\"}\n",
    "            \n",
    "            table.expires = expiration\n",
    "            table.description = description\n",
    "            table.labels = labels\n",
    "            table = BQ_CLIENT.update_table(table, [\"expires\",\"description\",\"labels\"])  # API request\n",
    "            \n",
    "            # Finally, return results\n",
    "            create_temporal_table_result['success'] = True\n",
    "            create_temporal_table_result['message'] = 'Temporal table was created'\n",
    "            create_temporal_table_result['temporal_table_id'] = temporal_table_id\n",
    "\n",
    "        except BadRequest as e:\n",
    "            create_temporal_table_result['message'] = 'ERROR: BigQuery Client BadRequest: {}'.format(e.message)\n",
    "\n",
    "        except:\n",
    "            create_temporal_table_result['message'] = 'Errors: {}'.format(sys.exc_info()[0])\n",
    "            \n",
    "    else:\n",
    "        print('User query has one or more execution warnings\\n{}'.format(dry_run_result.get('message')))\n",
    "        create_temporal_table_result['message'] = dry_run_result.get('message')\n",
    "\n",
    "    return create_temporal_table_result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del job de exportación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data_to_gcs(**kwargs):\n",
    "    \"\"\"\n",
    "        Generate and export data from a table in google bigquery to a bucket and prefix in google storage.\n",
    "        :param\n",
    "            bigquery_client (bigquery.client): \n",
    "            user_email (str):\n",
    "            source (str):\n",
    "            bucket (str):\n",
    "            prefix (str): \n",
    "            destination_format (str): \n",
    "                Specifies the type of file to be generated. The schema of the table that you want to export to GCS depends on this.\n",
    "                Tables with nested or repeated fields cannot be exported as CSV.\n",
    "                    'CSV' Specifies CSV format.\n",
    "                    'NEWLINE_DELIMITED_JSON' Specifies newline delimited JSON format.\n",
    "                    'PARQUET' Specifies Parquet format.\n",
    "                    'AVRO' Specifies Avro format.\n",
    "            \n",
    "            compression (str): \n",
    "                The compression type to use for exported files. The default value is NONE.\n",
    "                    'DEFLATE' Specifies DEFLATE format.\n",
    "                    'GZIP' Specifies GZIP format.\n",
    "                    'NONE' Specifies no compression.\n",
    "                    'SNAPPY' Specifies SNAPPY format.\n",
    "        :return\n",
    "            (dict) status and message of process\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # custom result dict\n",
    "    export_data_to_gcs_result = {'success':False,\n",
    "                                 'message':None,\n",
    "                                 'authenticated_url':None}\n",
    "    \n",
    "    # method arguments\n",
    "    BQ_CLIENT = kwargs.get('bigquery_client')\n",
    "    USER_EMAIL = kwargs.get('user_email')\n",
    "    SOURCE = kwargs.get('source')\n",
    "    BUCKET = kwargs.get('bucket')\n",
    "    PREFIX = kwargs.get('prefix')\n",
    "    DESTINATION_FORMAT = kwargs.get('destination_format','CSV').upper()\n",
    "    COMPRESSION = kwargs.get('compression')\n",
    "    \n",
    "    # Then, generate a new table, if exists drop it and create a new one\n",
    "    try:\n",
    "        table = BQ_CLIENT.get_table(SOURCE)\n",
    "        \n",
    "        table_size = table.num_bytes\n",
    "        print('table size is {}'.format(table_size))\n",
    "        \n",
    "    except NotFound as n:\n",
    "        export_data_to_gcs_result['message'] = 'Errors: Table {} does not exists \\n{}'.format(SOURCE ,n.message) \n",
    "        return export_data_to_gcs_result\n",
    "    except:\n",
    "        export_data_to_gcs_result['message'] = 'Errors: {}'.format(sys.exc_info()[0])\n",
    "        return export_data_to_gcs_result\n",
    "\n",
    "    assert (BQ_CLIENT and USER_EMAIL and SOURCE and BUCKET), 'One or more parameters are null'\n",
    "\n",
    "    try:\n",
    "        source_project = SOURCE.split('.')[0]\n",
    "        source_dataset = SOURCE.split('.')[-2]\n",
    "        source_table = SOURCE.split('.')[-1]\n",
    "        dataset_ref = bigquery.DatasetReference(source_project, source_dataset)\n",
    "        table_ref = dataset_ref.table(source_table)\n",
    "        \n",
    "        full_table_id = table_ref # generate a TableReference Object\n",
    "    except:\n",
    "        export_data_to_gcs_result['message'] = 'Errors: bigquery.TableReference.from_string \\n{}'.format(sys.exc_info()[0]) \n",
    "        return export_data_to_gcs_result\n",
    "    \n",
    "    # format user name\n",
    "    names = USER_EMAIL.split('@')[0].split('.')\n",
    "    user_name = ''.join([name.capitalize() for name in names])\n",
    "    \n",
    "    # Extract job labels\n",
    "    job_export_labels = {\n",
    "        \"user\": user_name.lower(),\n",
    "        \"source_dataset_id\": source_dataset.lower(),\n",
    "        \"source_table_id\": source_table.lower() \n",
    "    }\n",
    "    \n",
    "    # \n",
    "    extract_job_prefix = 'dq-test-job-export'\n",
    "    extract_job_location = 'US'\n",
    "    \n",
    "    # File .csv name, format -> export_result_<user>_<timestamp>\n",
    "    # Timestamp to temporal view name\n",
    "    current_timestamp = datetime.strftime(datetime.now(), \"%Y%m%d_%H%M00\")\n",
    "    current_timestamp_by_hour = datetime.strftime(datetime.now(), \"%Y%m%d_%H0000\")\n",
    "    \n",
    "    # URIs of Cloud Storage file(s) into which table data is to be extracted; in format\n",
    "    NEW_PREFIX = '{prefix}/{userfolder}/{hour}'.format(prefix=PREFIX, \n",
    "                                                       hour=current_timestamp_by_hour,\n",
    "                                                       userfolder=user_name)\n",
    "    user_destination_path = 'gs://{bucket}/{new_prefix}'.format(bucket=BUCKET, new_prefix=NEW_PREFIX)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.job.ExtractJobConfig.html#google.cloud.bigquery.job.ExtractJobConfig\n",
    "        extract_job_config = bigquery.job.ExtractJobConfig()\n",
    "        \n",
    "        # Set parameters to extract job\n",
    "        extract_job_config.destination_format = DESTINATION_FORMAT\n",
    "        extract_job_config.labels = job_export_labels\n",
    "          \n",
    "        if DESTINATION_FORMAT in ('CSV'):\n",
    "            extract_job_config.field_delimiter = ';'\n",
    "            extract_job_config.print_header = True\n",
    "            \n",
    "        # Generate URIs\n",
    "        if COMPRESSION:\n",
    "            extract_job_config.compression = COMPRESSION.upper()\n",
    "            filename_wildcard = 'export_result_{user}_{timestamp}_part_*.{file_format}.{compression}'.format(user=user_name,\n",
    "                                                                                                                     timestamp=current_timestamp,\n",
    "                                                                                                                     file_format=DESTINATION_FORMAT.lower(),\n",
    "                                                                                                                     compression=COMPRESSION.lower())\n",
    "            \n",
    "        else:\n",
    "            filename_wildcard = 'export_result_{user}_{timestamp}_part_*.{file_format}'.format(user=user_name,\n",
    "                                                                                                       timestamp=current_timestamp,\n",
    "                                                                                                       file_format=DESTINATION_FORMAT.lower()\n",
    "                                                                                                      )\n",
    "    except:\n",
    "        export_data_to_gcs_result['message'] = 'Error when try to create the ExtractJobConfig'\n",
    "    \n",
    "    print('generating file into GCS --> {}'.format(filename_wildcard))\n",
    "    \n",
    "    try:\n",
    "        # Start a job to extract a table into Cloud Storage files.\n",
    "        extract_job = BQ_CLIENT.extract_table(source=full_table_id,\n",
    "                                                destination_uris='{}/{}'.format(user_destination_path,filename_wildcard),\n",
    "                                                location=extract_job_location,\n",
    "                                                job_config=extract_job_config, \n",
    "                                                job_id_prefix=extract_job_prefix\n",
    "                                               )  # API request\n",
    "\n",
    "        extract_job.result()  # Waits for job to complete.\n",
    "        \n",
    "    except BadRequest as e:\n",
    "        export_data_to_gcs_result['message'] = 'ERROR: BigQuery Client BadRequest: {}'.format(e.message)\n",
    "    except:\n",
    "        export_data_to_gcs_result['message'] = 'Errors: {}'.format(sys.exc_info()[0])  \n",
    "         \n",
    "    ### Generate Compose File\n",
    "    \n",
    "    # Google Storage Client\n",
    "    try:\n",
    "        storage_client = storage.Client()\n",
    "        print('Storage Client OK')\n",
    "    except:\n",
    "        export_data_to_gcs_result['message'] = 'Errors: Unable to connect with Google Store API {}'.format(sys.exc_info()[0]) \n",
    "        \n",
    "    #\n",
    "        \n",
    "    try:\n",
    "        blobs = storage_client.list_blobs(bucket_or_name=BUCKET, prefix=NEW_PREFIX)\n",
    "\n",
    "        # list of files generated\n",
    "        list_files = []\n",
    "        list_blobs = []\n",
    "\n",
    "        # Loop into bucket using prefix \n",
    "        for blob in blobs:\n",
    "            list_files.append(blob.name)\n",
    "            list_blobs.append(blob)\n",
    "\n",
    "        compose_filename = filename_wildcard.replace('part_*','compose')\n",
    "        destination_prefix = '{}/{}'.format(NEW_PREFIX, compose_filename)\n",
    "\n",
    "        # Create compose filename\n",
    "        bucket = storage_client.bucket(BUCKET)\n",
    "        blob = bucket.blob(destination_prefix)\n",
    "\n",
    "        # Merge files into a one\n",
    "        destination = bucket.blob(destination_prefix)\n",
    "        destination.content_type = \"text/plain\"\n",
    "        destination.compose(list_blobs)\n",
    "\n",
    "        # Grant read permission to user\n",
    "        #result_blob = storage.Blob(destination_prefix, bucket)\n",
    "        destination.acl.user(USER_EMAIL).grant_read()\n",
    "        destination.acl.save()\n",
    "\n",
    "        #\n",
    "        # if everything is run correctly, it generates results message\n",
    "        url_result_file = 'https://storage.cloud.google.com/{bucket}/{prefix}'.format(bucket=BUCKET, prefix=destination_prefix)\n",
    "\n",
    "        export_data_to_gcs_result['success'] = True\n",
    "        export_data_to_gcs_result['message'] = 'file was generated successfully in GCS'\n",
    "        export_data_to_gcs_result['authenticated_url'] = url_result_file\n",
    "\n",
    "    except:\n",
    "        export_data_to_gcs_result['message'] = 'Errors: {}'.format(sys.exc_info()[0])  \n",
    "    \n",
    "        \n",
    "    return export_data_to_gcs_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "create_temporal_table_result =  create_temporal_table(user_email=USER_EMAIL, \n",
    "                      user_query=USER_QUERY, \n",
    "                      bigquery_client=BIGQUERY_CLIENT,\n",
    "                      temporal_project_id=TEMPORAL_PROJECT_ID, \n",
    "                      temporal_dataset_id=TEMPORAL_DATASET_ID)\n",
    "\n",
    "print(create_temporal_table_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "USER_EMAIL = 'guillermo.hernandez@pedidosya.com'\n",
    "export_data_to_gcs_results = export_data_to_gcs(bigquery_client=BIGQUERY_CLIENT, \n",
    "                   user_email= USER_EMAIL, \n",
    "                   source=create_temporal_table_result.get('temporal_table_id'), \n",
    "                   bucket='peya-anonymization-libraries', \n",
    "                   prefix='peya-data-export-repo', \n",
    "                   destination_format='AVRO',\n",
    "                   compression='GZIP'\n",
    "                  )\n",
    "\n",
    "print(export_data_to_gcs_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permisos de descarga del archivo al usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check \n",
    " # Job config to validate query billing, rows, slots\n",
    "job_test_config = bigquery.QueryJobConfig(dry_run=True,\n",
    "                                          use_query_cache=False)\n",
    "\n",
    "job_query_test = BIGQUERY_CLIENT.query(query=query_user,\n",
    "                                   job_id_prefix='DataTech_ExportDataService_DryRun_Query',\n",
    "                                       job_config=job_test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = job_query_test._properties['statistics']['query']['schema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = schema['fields']\n",
    "not_plain_columns = []\n",
    "for field in fields:\n",
    "    field_name = field['name']\n",
    "    field_type = field['type']\n",
    "    field_mode = field['mode']\n",
    "    if field_type in ('RECORD','ARRAY') or field_mode in ('REPEATED'):\n",
    "        not_plain_columns.append([field_name,field_type, field_mode])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_plain_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPORAL_PROJECT_ID = 'peya-data-qlty-stg'\n",
    "TEMPORAL_DATASET_ID = '_eca9662032d34636a2856a6fa78e751bd17ac6a1'\n",
    "\n",
    "temporal_table_name = 'anonc32e728581a24957c6c949c458067be6c408f7cb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update table metadata\n",
    "dataset_ref = bigquery.DatasetReference(TEMPORAL_PROJECT_ID, TEMPORAL_DATASET_ID)\n",
    "table_ref = dataset_ref.table(temporal_table_name)\n",
    "table = bigquery.Table(table_ref)\n",
    "\n",
    "query_user = 'SELECT * FROM `peya-bi-tools-pro.il_core.dim_partner` where 1=1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    table_ref = BIGQUERY_CLIENT.get_table('peya-data-qlty-stg.data_export_temporal.data_export_CarlosJaime_20210513_120400')\n",
    "except NotFound as n:\n",
    "    print('ERROR:  {}'.format(n.message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_plain_columns = []\n",
    "for field in table_ref.schema:\n",
    "    if field.field_type in ('RECORD','ARRAY'):\n",
    "        not_plain_columns.append([field.name,field.field_type])\n",
    "\n",
    "print(not_plain_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "print(str(uuid.uuid4())[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
